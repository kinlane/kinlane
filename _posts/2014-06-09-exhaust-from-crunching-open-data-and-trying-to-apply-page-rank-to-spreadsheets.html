---
layout: post
title: 'Exhaust From Crunching Open Data And Trying To Apply Page Rank To Spreadsheets'
source: http://kinlane.com/2014/06/09/exhaust-from-crunching-open-data-and-trying-to-apply-page-rank-to-spreadsheets/
domain: kinlane.com
image: http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/data-pagerank.png
---
<p><img style="padding: 15px;" src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/blog/data-pagerank.png" alt="" width="250" align="right" /></p>
<p>I stumbled across a very interesting post on <a href="http://dada.pink/dada/pagerank-for-spreadsheets/">pagerank for spreadsheets</a>. The post is a summary of a talk, but provided an interesting look at trying to understand open data at scale. Something I've tried doing several times, including my <a href="http://kinlane.com/2014/01/18/adopt-a-federal-government-dataset/">Adopt A Federal Government Dataset</a> work. Which reminds me of how horribly out of data it all is.</p>
<p>There is a shitload of data stored in Microsoft Excel, Google Spreadsheet and CSV files, and trying to understand where this data is, and what is contained in these little data stores is really hard. This post doesn&rsquo;t provide the answers, but gives a very interesting look into what goes into trying to understand open data at scale.</p>
<p>The author acknowledges something I find fascinating, that &ldquo;search for spreadsheet is hard&rdquo;&mdash;damn straight. He plays with different ways for quantifying the data based upon number columns, rows, content, data size and even file formats.</p>
<p>This type of storytelling from the trenches is very important. Every time I work to download, crunch and make sense of, or quantify open data, I try to tell the story in real-time. This way much of the mental exhaust from the process is public, potentially saving someone else some time, or helping them see it through a different lens.</p>
<p>Imagine if someone made the Google, but just for public spreadsheets. Wish I had a clone!</p>